---
title: 2024-01-15 Recaps
lang: en-US
---

# 2024-01-15 Recaps

## GCP

- [2023 Recap](https://cloud.google.com/blog/products/gcp/google-cloud-top-news-of-2023?utm_source=substack&utm_medium=email): Pace. Never. Ends.
- [LangCahin](https://cloud.google.com/blog/products/databases/using-pgvector-llms-and-langchain-with-google-cloud-databases): LangChain is the story of 2023
- [Vertex AI & DataFlow](https://cloud.google.com/blog/products/ai-machine-learning/streaming-prediction-with-dataflow-and-vertex?utm_source=substack&utm_medium=email): All roads lead to Vertex AI
- [Duet AI - GCP Assistant](https://cloud.google.com/blog/products/ai-machine-learning/get-ai-help-on-networking-tasks?utm_source=substack&utm_medium=email): Duet AI is a great tool
- [Container Management](https://cloud.google.com/blog/products/containers-kubernetes/a-leader-in-2023-gartner-magic-quadrant-for-container-management) GCP leading the way per Gartner
- [MedLM](https://cloud.google.com/blog/topics/healthcare-life-sciences/introducing-medlm-for-the-healthcare-industry) Consolidating domain for world's knowledge
- [Document Extraction](https://cloud.google.com/blog/products/ai-machine-learning/document-ai-custom-extractor-powered-by-generative-ai-is-now-ga?utm_source=substack&utm_medium=email) It's all getting too easy

## AI Frontier

- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory): Fine-tuning Open Source Models
  - [DPO](####DPO):
- [MISTRAL AI](https://mistral.ai/): Committing to open models
  - [Mixtral 8x7B](https://mistral.ai/news/mixtral-of-experts/): A 7B sparse Mixture-of-Experts model with stronger capabilities than Mistral 7B. Uses 12B active parameters out of 45B total. Supports multiple languages, code and 32k context window
- [mergekit](https://github.com/cg123/mergekit): toolkit for merging pre-trained language models.
  - [tutorial](https://freedium.cfd/https://towardsdatascience.com/merge-large-language-models-with-mergekit-2118fb392b54)

## AI Companies

- [datagpt](https://datagpt.com/): I believe this is the future of Data Analysis. AI agents scanning behind the scene surfacing insights to be interpreted

## ML/DE Engineering

- [Data Engineering Handbook](https://github.com/DataEngineer-io/data-engineer-handbook): looks comprehensive
- [Testing in MLOps](https://freedium.cfd/https://towardsdatascience.com/testing-in-practice-code-data-and-ml-model-cfb1ada81f6c)

## Rusts

- [Enums](https://www.shuttle.rs/blog/2023/11/23/enums-in-rust): Agree with this.
- [Editions](https://doc.rust-lang.org/stable/edition-guide/editions/index.html): Support features for all future releases. Powerful idea.

## Vue + Web

[URL Design](https://blog.jim-nielsen.com/2023/examples-of-great-urls/): Fun, Simple & Intuitive

## Python

[libraries of 2023](https://tryolabs.com/blog/top-python-libraries-2023)

- LiteLLM
- [PyApp](https://github.com/ofek/pyapp)
- Taipy
- Unstructured
- [Google's Automlops](https://github.com/GoogleCloudPlatform/automlops)
- [LLM guardrails](https://github.com/guardrails-ai/guardrails)
- [LLM's outlines](https://github.com/outlines-dev/outlines)
- [LLM's prompt tools](https://github.com/hegelai/prompttools)
- [temporian](https://github.com/google/temporian)
- [functime](https://github.com/functime-org/functime)

### Definitions (Thanks Bard)

#### DPO:

In the context of fine-tuning large language models (LLMs), DPO stands for Direct Preference Optimization. It's an alternative approach to traditional reinforcement learning (RL) methods for adapting LLMs to specific tasks or preferences.

Here's a breakdown of DPO:

Objective: Unlike RL, which often relies on complex reward functions, DPO's goal is simple - directly maximize the likelihood of LLM outputs that humans prefer.
Training data: Instead of interacting with the LLM to provide feedback, DPO uses datasets of preference pairs. Each pair consists of a prompt and two possible completions, one preferred and one dispreferred.
Optimization: During training, the LLM parameters are adjusted to increase the probability of generating the preferred outputs across these pairs.
Here are some advantages of DPO:

Simpler and faster: It avoids the complexities of designing and training reward models, making it easier to implement and computationally efficient.
More stable: DPO training tends to be more stable than RL, minimizing performance fluctuations and unexpected outcomes.
Directly aligns with human preferences: By focusing on human judgment, DPO helps tailor the LLM's outputs to better meet specific needs and preferences.
However, DPO also has limitations:

Requires high-quality preference data: Collecting and labeling such data can be costly and time-consuming.
May not handle complex tasks as well: For intricate tasks with non-obvious preferences, RL methods might still offer better adaptability.
Less flexibility compared to RL: DPO directly optimizes for specific preferred outputs, while RL allows for exploring broader ranges of behavior.
Overall, DPO represents a promising approach for fine-tuning LLMs, especially when speed, stability, and direct alignment with human preferences are crucial. Its future development and applications hold exciting potential for various LLM applications.

If you have any further questions about DPO or any other aspect of LLM fine-tuning, feel free to ask! I'm happy to help.
